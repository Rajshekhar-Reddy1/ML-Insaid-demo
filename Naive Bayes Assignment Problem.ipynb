{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive Bayes Assignment Problem.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-WIIx3JHkO-O"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"sh62_W0okP9w"},"source":["**<center><h3>Naive Bayes Assignment Problem</h3></center>**"]},{"cell_type":"markdown","metadata":{"id":"GtaKdypWkYMQ"},"source":["---\n","# **Table of Contents**\n","---\n","\n","**1.** [**Problem Statement**](#Section1)<br>\n","**2.** [**Objective**](#Section2)<br>\n","**3.** [**Installing & Importing Libraries**](#Section3)<br>\n","  - **3.1** [**Installing Libraries**](#Section31)\n","  - **3.2** [**Upgrading Libraries**](#Section32)\n","  - **3.3** [**Importing Libraries**](#Section33)\n","\n","**4.** [**Data Acquisition & Description**](#Section4)<br>\n","  - **4.1** [**Data Description**](#Section41)\n","  - **4.2** [**Data Information**](#Section42)\n","\n","**5.** [**Data Pre-processing**](#Section5)<br>\n","  - **5.1** [**Pre-Profiling Report**](#Section51)<br>\n","\n","**6.** [**Exploratory Data Analysis**](#Section6)<br>\n","**7.** [**Post Data Processing & Feature Selection**](#Section7)<br>\n","  - **7.1** [**Bag of Words**](#Section71)<br>\n","  - **7.2** [**Count Vectorizer**](#Section72)<br>\n","  - **7.3** [**Data Preparation**](#Section73)<br>\n","\n","**8.** [**Model Development & Evaluation**](#Section8)<br>\n","**9.** [**Conclusion**](#Section9)<br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9k4rz6p7keYY"},"source":["---\n","<a name = Section1></a>\n","# **1. Problem Statement**\n","---\n","\n","- The **SMS Spam** Collection v.1 is a public set of SMS **labeled messages** that have been collected for **mobile phone spam** research.\n","\n","- It has one collection composed by **5,574** English, real and non-enconded **messages**, tagged according being legitimate (ham) or spam.\n","\n","- Identification of Ham/Spam is an **interesting usecase** of **NLP**(Natural Language Processing)."]},{"cell_type":"markdown","metadata":{"id":"f2bUZrCqkhfh"},"source":["---\n","<a name = Section2></a>\n","# **2. Objective**\n","---\n","\n","- The objective of this assignment is to **predict** the likeliness of **email** being **spam or ham**."]},{"cell_type":"markdown","metadata":{"id":"K1Dc1X67kkGo"},"source":["---\n","<a name = Section3></a>\n","# **3. Installing & Importing Libraries**\n","---"]},{"cell_type":"markdown","metadata":{"id":"chnc8GQHkmf6"},"source":["<a name = Section31></a>\n","### **3.1 Installing Libraries**"]},{"cell_type":"code","metadata":{"id":"IUq6i8xTbIm7"},"source":["!pip install -q datascience                   # Package that is required by pandas profiling\n","!pip install -q pandas-profiling              # Library to generate basic statistics about data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeK-S4CSko6y"},"source":["<a name = Section32></a>\n","### **3.2 Upgrading Libraries**\n","\n","- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync. \n","\n","- Make sure not to execute the cell above (3.1) and below (3.2) again after restarting the runtime."]},{"cell_type":"code","metadata":{"id":"wYsXt6jfkrtJ"},"source":["!pip install -q --upgrade pandas-profiling"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1hdcYUBktjK"},"source":["<a name = Section33></a>\n","### **3.3 Importing Libraries**"]},{"cell_type":"code","metadata":{"id":"DFiF1yPqkvZ7"},"source":["#-------------------------------------------------------------------------------------------------------------------------------\n","import pandas as pd                                                 # Importing for panel data analysis\n","from pandas_profiling import ProfileReport                          # Import Pandas Profiling (To generate Univariate Analysis) \n","pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high      \n","pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity      \n","pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n","pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n","#-------------------------------------------------------------------------------------------------------------------------------\n","import numpy as np                                                  # Importing package numpys (For Numerical Python)\n","import string                                                       # For string related operations\n","import pprint                                                       # For printing of Collections line by line\n","from collections import Counter                                     # For estimating frequency\n","#-------------------------------------------------------------------------------------------------------------------------------\n","import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib                                              \n","import seaborn as sns                                               # Importin seaborm library for interactive visualization\n","%matplotlib inline\n","#-------------------------------------------------------------------------------------------------------------------------------\n","from sklearn.feature_extraction.text import CountVectorizer         # To perform bag of words from data\n","from sklearn.metrics import precision_recall_curve                  # For precision and recall metric estimation\n","from sklearn.metrics import classification_report                   # To generate complete report of evaluation metrics\n","from sklearn.metrics import plot_confusion_matrix                   # To plot confusion matrix \n","#-------------------------------------------------------------------------------------------------------------------------------\n","from sklearn.model_selection import train_test_split                # To split the data in training and testing part     \n","from sklearn.naive_bayes import MultinomialNB                       # To create a naive bayes model using algorithm\n","#-------------------------------------------------------------------------------------------------------------------------------\n","import warnings                                                     # Importing warning to disable runtime warnings\n","warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOZDGO_bk7ty"},"source":["---\n","<a name = Section4></a>\n","# **4. Data Acquisition & Description**\n","---\n","\n","- This corpus has been collected from free for research sources at the Internet and it can be retrieved from the attached <a href = \"https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/Assignment/spam.csv\">**link**</a>.\n","\n","| Records | Features | Dataset Size |\n","| :-- | :-- | :-- |\n","| 5572 | 5 | 491 KB| \n","\n","|Id|Feature|Description|\n","|:--|:--|:--|\n","|01|**v1**|Whether an email is ham or spam. Contains: [ham, spam]|\n","|02|**v2**|It is the email message sent to and fro between sender and receiver.|\n","|03|**Unnamed: 2**|Irrelevant and Un-necessary feature.|\n","|04|**Unnamed: 3**|Irrelevant and Un-necessary feature.|\n","|05|**Unnamed: 4**|Irrelevant and Un-necessary feature.|"]},{"cell_type":"code","metadata":{"id":"EkahkxOVwXnX"},"source":["data = pd.read_csv('https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/Assignment/spam.csv', encoding = 'latin1')\n","print('Data Shape:', data.shape)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5AyNVVsyFHg"},"source":["**Observation:**\n","\n","- Before diving further let's **rename** the **features** and **drop un-necessary features**.\n","\n","- Additionally, we will create a **new feature** out of the Message **length**."]},{"cell_type":"code","metadata":{"id":"jmOCGgiDEn6f"},"source":["def modify_data(data):\n","  # Enter your code here...\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TUuss7DUyN3z"},"source":["data = modify_data(data)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8_sCdncEgw4"},"source":["<a name = Section41></a>\n","### **4.1 Data Description**\n","\n","- In this section we will get **description** and **statistics** about the data."]},{"cell_type":"code","metadata":{"id":"Dz4XNvCP2A8h"},"source":["data.describe().transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Et2oxpEW6xG"},"source":["<a name = Section42></a>\n","### **4.2 Data Information**\n","\n","- In this section we will get **information about the data** and see some observations."]},{"cell_type":"code","metadata":{"id":"tNKHxPIkFT_I"},"source":["data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhALq5osFeDX"},"source":["<a name = Section5></a>\n","\n","---\n","# **5. Data Pre-Processing**\n","---"]},{"cell_type":"markdown","metadata":{"id":"IN6i2Yb6GYe_"},"source":["<a name = Section51></a>\n","### **5.1 Pre Profiling Report**\n","\n","- For **quick analysis** pandas profiling is very handy.\n","\n","- Generates profile reports from a pandas DataFrame.\n","\n","- For each column **statistics** are presented in an interactive HTML report."]},{"cell_type":"code","metadata":{"id":"Gdk4lCT2Fa4T"},"source":["profile = ProfileReport(df = data)\n","profile.to_file(output_file = 'Pre Profiling Report.html')\n","print('Accomplished!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbFnV0j3Gavb"},"source":["# from google.colab import files                   # Use only if you are using Google Colab, otherwise remove it\n","# files.download('Pre Profiling Report.html')      # Use only if you are using Google Colab, otherwise remove it"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Djt4cuEURFr6"},"source":["**Performing Operations**"]},{"cell_type":"code","metadata":{"id":"Z0_YwZDIRTn6"},"source":["def handle_duplicates(data):\n","\n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddv9utusQ9rN"},"source":["handle_duplicates(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaASnXjbRLWz"},"source":["**After Handling Duplicate Data**"]},{"cell_type":"code","metadata":{"id":"dorXnRd5RK7C"},"source":["print('Contains Redundant Records?:', data.duplicated().any())\n","print('Duplicate Count:', data.duplicated().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHzIs7h7SWhB"},"source":["<a name = Section6></a>\n","\n","---\n","# **6. Exploratory Data Analysis**\n","---"]},{"cell_type":"markdown","metadata":{"id":"vtSLhSHkUfBN"},"source":["---\n","**<h4>Question 1:** Plot the distribution of message length i.e. \"Length\".</h4>\n","\n","---"]},{"cell_type":"code","metadata":{"id":"zqoV_ACcSXdg"},"source":["def plotLenght():\n","\n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVC3wvdPIioo"},"source":["plotLenght()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOfw9p1SVaHZ"},"source":["<a name = Section7></a>\n","\n","---\n","# **7. Post Data Processing**\n","---\n","\n","- Now we will **transform** our **data** into **compatible format** so that machine can understand.\n","\n","- Our **data** set contains **5572** number of **rows** and these will be converted into **bag of words**.\n","\n","- But before that we need to **clean** the **text message** by \n","  \n","  - Removing stopwords, punctuations, delimeters, alphanumerics,\n","  - Converting all the words in lowercase.\n","\n","- We will then create a **vector space** of words as features and their row cells will contains the respective value (1 or 0).\n","\n","- We then use the **tokenized words** for each observation and find out the **frequency** of **each token**.\n","\n","- Example: Lets say we have 4 documents as follows:\n","\n","  ```\n","  Hello, how are you!\n","  ```\n","  ```\n","  Win money, win from home\n","  ```\n","  ```\n","  Call me now\n","  ```\n","  ```\n","  Hello, Call you tomorrow?\n","  ```\n","\n","- Our objective here is to convert this set of text to a frequency distribution matrix, as follows:\n","\n","<center><img src=\"https://image.ibb.co/casG7U/countvectorizer.png\" width=70%, height=70%></center>"]},{"cell_type":"markdown","metadata":{"id":"N8hdBEjZSLmD"},"source":["<a name = Section71></a>\n","## **7.1 Bag of Words**"]},{"cell_type":"markdown","metadata":{"id":"uO2DDflVZux3"},"source":["<a name = Section711></a>\n","### **7.1.1 Lowecase Transformation**"]},{"cell_type":"markdown","metadata":{"id":"3CnABpsSyHB6"},"source":["---\n","**<h4>Question 2:** Convert all the words in lower case for the give document.</h4>\n","\n","---"]},{"cell_type":"code","metadata":{"id":"o6ENU554yTHG"},"source":["documents = ['Hello, how are you!', \n","             'Win money, win from home.', \n","             'Call me now.', \n","             'Hello, Call hello you tomorrow?']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3HWD9hYymif"},"source":["- Create a function that return all the words in lower case."]},{"cell_type":"code","metadata":{"id":"hZ7YxGwmZ-hi"},"source":["def lowercase(doc):\n","    # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADOeRPYNysof"},"source":["- Call the function by passing the give document and print it's result."]},{"cell_type":"code","metadata":{"id":"Wn1P69X5RtxD"},"source":["lowerCaseDoc = lowercase(doc=documents)\n","print(lowerCaseDoc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_aiM9nuab-y"},"source":["<a name = Section712></a>\n","### **7.1.2 Removing All Punctuations**\n"]},{"cell_type":"markdown","metadata":{"id":"AztvRP1Zy4mW"},"source":["---\n","**<h4>Question 3:** Remove all the punctuations in the lowercase transformed document.</h4>\n","\n","---\n","\n","- Create a function that returns the documents contents without punctuations."]},{"cell_type":"code","metadata":{"id":"qtr2w2T9a6rV"},"source":["def removePunctuation(doc):\n","\n","  # Enter your code here...\n","\n","  return nonPunch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JeQeAfZgzSJb"},"source":["- Call the function by passing the lowercase transformed document and print it's result."]},{"cell_type":"code","metadata":{"id":"0K6Q9KMkaniO"},"source":["removePunch = removePunctuation(doc=lowerCaseDoc)\n","print(removePunch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yqfcrs8nbbyT"},"source":["<a name = Section713></a>\n","### **7.1.3 Tokenization**\n"]},{"cell_type":"markdown","metadata":{"id":"dkiDSNOuzebR"},"source":["---\n","**<h4>Question 4:** Perform tokenization over the document on which you just removed all the punctuations.</h4>\n","\n","---\n","\n","- Create a function that returns the contents of documents as tokens."]},{"cell_type":"code","metadata":{"id":"AT-e6M9wbt5D"},"source":["def tokenize(doc):\n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhqAyx580LPA"},"source":["- Call the function by passing the punctuation removed document and print it's result."]},{"cell_type":"code","metadata":{"id":"sowwlpH-bk9r"},"source":["tokenDoc = tokenize(doc=removePunch)\n","print(tokenDoc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TVe_elmdvHc"},"source":["<a name = Section714></a>\n","### **7.1.4 Count Frequencies**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hkHotas10btC"},"source":["---\n","**<h4>Question 5:** Estimate the frequencies of each token with the help of Counter method defined in Collections library.</h4>\n","\n","---\n","\n","- Create a function that returns the count frequency of documents."]},{"cell_type":"code","metadata":{"id":"AEJ4f7fsd__L"},"source":["def CountFreq(doc):\n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J7DbTISs0zE4"},"source":["- Call the function by passing the tokenized document and print it's result."]},{"cell_type":"code","metadata":{"id":"nSRKvOH8dpGb"},"source":["freqDoc = CountFreq(doc=tokenDoc)\n","pprint.pprint(freqDoc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTrzi10me2w2"},"source":["<a name = Section72></a>\n","## **7.2 Count Vectorizer**\n","\n","- In the above section you **saw** a version of the **CountVectorizer(**) method **from** the **scratch**.\n","\n","- But we can do **similar steps** with the help of CountVectorizer() with **one line** of **code**.\n","\n","- CountVectorizer() has certain parameters which take care of these steps for us which are as follows:\n","\n","  ```\n","  lowercase: The lowercase parameter has a default value of True which converts all of our text to its lower case form.\n","  ```\n","  ```\n","  token_pattern: The token_pattern parameter has a default regular expression value of - (?u)\\\\b\\\\w\\\\w+\\\\b which ignores \n","                 all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length \n","                 greater than or equal to 2, as individual tokens or words.\n","  ```\n","  ```\n","  stop_words: The stop_words parameter, if set to english will remove all words from our document set that match a list of \n","              English stop words which is defined in scikit-learn. \n","  ```\n"]},{"cell_type":"markdown","metadata":{"id":"dQs-Vlob07-4"},"source":["---\n","**<h4>Question 6:** Perform the count vectorization over the documents feature created earlier.</h4>\n","\n","---\n","\n","- Create a feature as an object for CountVectorizer() and pass the respective parameters.\n","\n","- **Fit** and **transform** the **data** to an array and finally **create a dataframe** that show the **frequency matrix**.\n","\n","- Make sure that the **frequency matrix** **show** the **labels** of the messsages as features."]},{"cell_type":"code","metadata":{"id":"2xTDuZ0uis-Y"},"source":["def countVectorTransform(data, stopwords=None):\n","\n","  # Enter your code here...\n","\n","  return frequency_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_m5pmMF2n8T"},"source":["- Call the function by passing the original document variable that was created earlier and print it's result."]},{"cell_type":"code","metadata":{"id":"RyqSh2IYh4bK"},"source":["frequency_matrix = countVectorTransform(data = documents)\n","frequency_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZdbwunqWl8Pj"},"source":["<a name = Section73></a>\n","## **7.3 Data Preparation**\n","\n","- Now we will **split** our **data** in **training** and **testing** part for further development.\n","\n","- Before that you need to **convert** the **class labels** of Label features into **numeric**."]},{"cell_type":"markdown","metadata":{"id":"DO305pCo3H67"},"source":["---\n","**<h4>Question 7:** Perform the count vectorization over the message feature.</h4>\n","\n","---\n","\n","- **Perform label encoding** over Label feature present in data.\n","\n","- Take **count vectorizer transformation** over the message feature as input and Label feature as output.\n","\n","- **Split** the data into **80:20** along with the **stratify** parameter inside train_test_split.\n","\n","- Make sure to set the **random_state = 42**."]},{"cell_type":"code","metadata":{"id":"2i4i7Mal311K"},"source":["def dataPrep():\n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sid1Rpgso-lI"},"source":["dataPrep()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6b_-M1bBnSM_"},"source":["<a name = Section8></a>\n","\n","---\n","# **8. Model Development & Evaluation**\n","---\n","\n","- In this section we will **develop Multinomial Naive Bayes** and **tune** our **model if required**.\n","\n","- Then we will **analyze the results** obtained and **make our observation**.\n","\n","- For **evaluation purpose** we will **focus** on **precision and recall value** for **positive class** i.e. spam class.\n","\n","- **Remember** that **we want generalize results** i.e. same results or error on testing data as that of training data."]},{"cell_type":"code","metadata":{"id":"UEBRt2hUnlI9"},"source":["def plot_precision_recall(y_true, y_pred, train_or_test):\n","  '''\n","  y_true: Acutal values of the target\n","  y_pred: Predicted values of the target. Either predict_proba or decision_function\n","  line_show: Plot avergae values \"precision\" or \"recall\"\n","  train_or_test: Train Data or Test Data\n","  '''\n","  precisions, recalls, thresholds = precision_recall_curve(y_true = y_true, probas_pred = y_pred)\n","\n","  average_precision = np.mean(precisions)\n","  average_recall = np.mean(recalls)\n","\n","  sns.lineplot(x = recalls, y = precisions, linewidth = 2, ci = None)\n","  plt.plot([0, 1], [average_precision, average_precision], 'r-')\n","  plt.plot([average_recall, average_recall], [0, 1], 'g-')\n","  plt.xlabel('Recall', fontsize = 14)\n","  plt.xticks(ticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n","  plt.yticks(ticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n","  plt.ylabel('Precision', fontsize = 14)\n","  plt.title(train_or_test, fontsize = 16)\n","  plt.legend(labels = ['Binary PR Curve', 'AP {:.2f}'.format(average_precision), 'AR {:.2f}'.format(average_recall)])\n","  plt.grid(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpjUN1pc4XLU"},"source":["---\n","**<h4>Question 8:** Perform model building using Multinomial Naive Bayes over training data using default setting.</h4>\n","\n","---"]},{"cell_type":"code","metadata":{"id":"eYohpjMVvkf-"},"source":["def modelFit():\n","  \n","  # Enter your code here...\n","\n","  return naive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pg0qvO-FvvBe"},"source":["model = modelFit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HwwITXJo4iqT"},"source":["---\n","**<h4>Question 9:** Generate a confusion matrix over the training data as well as on testing data.</h4>\n","\n","---"]},{"cell_type":"code","metadata":{"id":"9CfAED9WpblC"},"source":["def plotConf():\n","\n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrqdSqUpv2Fe"},"source":["plotConf()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yuc12PXVnvTX"},"source":["def plotReport():\n","  \n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Ob1WFVd4xpL"},"source":["---\n","**<h4>Question 10:** Generate a classification report for training data as well as on testing data.</h4>\n","\n","---"]},{"cell_type":"code","metadata":{"id":"luATtr1XwUi_"},"source":["plotReport()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56qXGjUm4-mL"},"source":["---\n","**<h4>Question 11:** Generate a precision and recall curve for training data as well as on testing data.</h4>\n","\n","---"]},{"cell_type":"code","metadata":{"id":"tnWf17gMnz-c"},"source":["def plotPRCurve():\n","  \n","  # Enter your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6MOMBVmwc4O"},"source":["plotPRCurve()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6oqv0vpcXhJp"},"source":["<a name = Section9></a>\n","\n","---\n","# **9. Conclusion**\n","---"]},{"cell_type":"markdown","metadata":{"id":"3p0byDJaXnB5"},"source":["- We can see that **model** is **working pretty well** in **identifying** **spam** messages.\n","\n","- This **model** now can **help us** in **identifying** which **message** is **spam** and which one is not."]}]}